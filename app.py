import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st
from unite_toolbox.knn_estimators import calc_knn_density


def calc_nse(obs: np.array, sim: np.array):
    return 1 - np.sum((obs - sim) ** 2) / np.sum((obs - np.mean(obs)) ** 2)

class PowerReservoir:
    def __init__(self, params=None):
        self.parameters = {"a": 0.8, "b": 1.2, "k": 24.0}
        if params is not None:
            self.parameters["a"] = params["a"]
            self.parameters["b"] = params["b"]
            self.parameters["k"] = params["k"]

    def forward(self, p, et):
        a = self.parameters["a"]
        b = self.parameters["b"]
        k = self.parameters["k"]

        si = 0.001
        s, q = np.empty(len(p)), np.empty(len(p))
        for idx in range(len(p)):
            ret = a * et[idx]
            q_in = max(p[idx] - ret, 0)
            si = si + q_in
            q_out = min((si ** b) / k, si)
            si = si - q_out
            s[idx], q[idx] = si, q_out
        return s, q


def f(a, b, k) -> np.array:
    pr = PowerReservoir({"a": a, "b": b, "k": k})
    _, q = pr.forward(p, et)
    return q


data = pd.read_csv("data.csv", index_col=0, parse_dates=True)
qobs = data["qobs"].values
p = data["prcp"].values
et = data["eto"].values

# Dashboard
st.title("Uncertainty Metrics Dashboard")

# Create sliders for parameters
a = st.slider(
    r"$\alpha$",
    min_value=0.5,
    max_value=3.0,
    value=1.0,
    step=0.1,
    help="Multiplicative factor for evapotranspiration.",
)

b = st.slider(
    r"$\beta$",
    min_value=0.5,
    max_value=2.0,
    value=1.0,
    step=0.1,
    help="Exponential component of the outflow relationship.",
)

k = st.slider(
    r"$k$",
    min_value=1.0,
    max_value=50.0,
    value=19.0,
    step=1.0,
    help="Linear component of the outflow relationship.",
)

sigma = st.slider(
    r"$\sigma$",
    min_value=0.1,
    max_value=4.0,
    value=1.0,
    step=0.1,
    help="Standard deviation of the noise.",
)

button = st.toggle("Heteroskedastic")



pw = PowerReservoir()
s, qobs = pw.forward(p, et)

qsim = f(a, b, k)

# NSE
nse = calc_nse(qobs, qsim)

# Log pointwise predictive density
qsim = np.tile(qsim.reshape(-1, 1), 1000)
if button:
    qsim = qsim + qsim * np.random.normal(0, sigma, qsim.shape) / 5.0
else:
    qsim = qsim + np.random.normal(0, sigma, qsim.shape)

qsim_median = np.median(qsim, axis=1).flatten()
qsim_lower = np.quantile(qsim, 0.05, axis=1).flatten()
qsim_upper = np.quantile(qsim, 0.95, axis=1).flatten()
loglik = 0.0
for idx in range(len(qsim_median)):
    loglik += np.log(calc_knn_density(qobs[idx].reshape(-1, 1), qsim[idx, :].reshape(-1, 1)))[0]


num_days = len(pd.date_range(start="1999-04-01", end="1999-10-01", freq="D"))
dates = data.index[-num_days:]

col1, col2 = st.columns(2)
col1.metric("NSE", f"{nse:.3f}")
col2.metric("Log-likelihood", f"{loglik:.0f}")

# Plot
fig, ax = plt.subplots()

# Observed
ax.plot(dates, qobs[-num_days:], color="tab:blue", linewidth=2, alpha=0.5, label="Observed")

# Simulated
ax.plot(dates, qsim_median[-num_days:], color="tab:green", linewidth=2, label="Simulated")
ax.fill_between(dates, qsim_lower[-num_days:], qsim_upper[-num_days:], alpha=0.3, label="Prediction CI [5%, 95%]", color="tab:green")

ax.set_ylim(-0.5, 10.5)
ax.set_xlabel("Date")
ax.set_ylabel("Streamflow [mm/day]")
ax.legend()
ax.grid(True)

# Display the plot in Streamlit
st.pyplot(fig)

md = r"""
# Notes
## Model
In this dashboard you can adjust the parameters of a simple power reservoir model
and see how the simulations of streamflow change. The model is defined as:

### Equations
$$\frac{dS}{dt} = P - \alpha\,ET - Q$$

$$Q = \frac{S^\beta}{k}$$

Where:
- $S$ is the storage of the reservoir [in mm]
- $P$ is the input precipitation [in mm/day]
- $ET$ is the potential evapotranspiration [in mm/day]
- $\alpha$ is a multiplicative factor for evapotranspiration [-]
- $\beta$ is the exponent of the outflow relationship [-]
- $k$ is the linear component of the outflow relationship [-]
- $Q$ is the outflow [in mm/day]

The 'observed' data is generated by a 'true' model that uses the parameters:

$$\alpha=0.8\quad\beta=1.2\quad k=24.0$$

See if you can find a different set of parameters that also match the data (equifinality)!

### Noise
The model also considers a noise term as a proxy for the uncertainty in the simulations.
Noise is propagated through the model using the following equation:

$$Q = Q + \epsilon\,\mathcal{N}(0, \sigma)$$

Where:
- $\epsilon$ is a equal to $Q$ if the heteroskedastic toggle is activated, and 1.0 otherwise
- $\sigma$ is the standard deviation of the noise [in mm/day]

Additionaly, in heteroskedastic mode, the noise is divided by 5.0 to make it less influential
considering that it's multiplied by the simulated streamflow, but this is just a stylistic choice.

## Metrics
At the bottom of the dashboard you can see some metrics. Here we give a brief explanation
for each of them.

### NSE
The Nash-Sutcliffe Efficiency (NSE) is a metric that compares the observed streamflow with the
simulations from the model. It is defined as:

$$NSE = 1 - \frac{\sum(Q_{obs} - Q_{sim})^2}{\sum(Q_{obs} - \overline{Q}_{obs})^2}$$

The numerator of the equation is similar to mean-squared error. NSE is better when it's closer to 1.

### Log-likelihood
The log-likelihood is a measure of how well the model fits the data. It can be described as the probability density
of the observed data $y_{obs}$ under the model $\hat{p}(\cdot)$.

$$\sum \log \hat{p}(y_{obs})$$

Because often the predictive distribution of the model is not available in closed form,
for this dashboard we use the [UNITE toolbox](https://github.com/manuel-alvarez-chaves/unite_toolbox),
to approximate $\hat{p}(y_{obs})$ using a k-nearest neighbors estimator.

$$\hat{p}_k(y) = \frac{k}{N-1} \cdot \frac{1}{c_1(d)\;\rho^{d}_k(i)}$$

Where:
- $k$ is the number of neighbors.
- $N$ is the number of simulated samples.
- $c_1(d)$ is the volume of a d-dimensional unit $L^p$ ball.
- $d$ is the number of dimensions of the data.
- $\rho^{d}_k(i)$ is the distance between a point $i$ and its *k*-th nearest neighbor.

Importantly: $\rho^{d}_k(i)$ is evaluated as the distance between the observed value $y_{obs}$ and the *k*-th nearest neighbor in the
simulated samples $y_{sim}$. This is done for each observation in the dataset.

## References
- Gelman, A., Hwang, J., & Vehtari, A. (2013). Understanding predictive information criteria for Bayesian models (arXiv:1307.5928). arXiv. https://doi.org/10.48550/arXiv.1307.5928
- Wang, Q., Kulkarni, S. R., & Verdu, S. (2009). Divergence Estimation for Multidimensional Densities Via k-Nearest-Neighbor Distances. IEEE Transactions on Information Theory, 55(5), 2392â€“2405. IEEE Transactions on Information Theory. https://doi.org/10.1109/TIT.2009.2016060
"""
st.markdown(md)
